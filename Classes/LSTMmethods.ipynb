{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyPe2Z+iUVPOQ73VSACIuQTX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Train a LSTM (RNN) to make predictions based on specific train and validation sets"],"metadata":{"id":"smPB1gwQtXCQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vjx60ntBha1W","executionInfo":{"status":"ok","timestamp":1714200075202,"user_tz":-180,"elapsed":339374,"user":{"displayName":"Roumeliotis Konstadinos","userId":"17264923090131634662"}},"outputId":"28ece510-6191-446d-dfe8-612700b2117c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 13819575463734692822\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 15510929408\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 10752727187895355201\n","physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n","xla_global_id: 416903419\n","]\n","Mounted at /content/gdrive\n","Epoch 1/3\n","611/611 [==============================] - 123s 195ms/step - loss: 0.5987 - accuracy: 0.7498 - val_loss: 0.5508 - val_accuracy: 0.7612\n","Epoch 2/3\n","611/611 [==============================] - 95s 156ms/step - loss: 0.5506 - accuracy: 0.7613 - val_loss: 0.5529 - val_accuracy: 0.7612\n","Epoch 3/3\n","611/611 [==============================] - 84s 138ms/step - loss: 0.5506 - accuracy: 0.7613 - val_loss: 0.5512 - val_accuracy: 0.7612\n"]}],"source":["import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense # Î™nclude LSTM layer\n","from tensorflow.python.client import device_lib\n","import time\n","from google.colab import drive\n","import os\n","\n","# Check available devices\n","devices = device_lib.list_local_devices()\n","print(devices)\n","\n","class LSTMTraining:\n","    def __init__(self, learning_rate, epochs, batch_size, max_len, feature_col, label_col):\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.max_len = max_len\n","        self.feature_col = feature_col\n","        self.label_col = label_col\n","        self.history = None\n","\n","    def load_data(self, train_file_path, val_file_path):\n","        drive.mount('/content/gdrive') # Mount Google Drive to access files\n","        self.train_df = pd.read_csv(train_file_path) # Store data frames as instance variables\n","        self.val_df = pd.read_csv(val_file_path)\n","\n","    def preprocess_data(self):\n","        self.tokenizer = Tokenizer() # Initializing Tokenizer\n","        self.tokenizer.fit_on_texts(self.train_df[self.feature_col]) # Fitting tokenizer on training text data\n","\n","        # Converting text data to sequences\n","        train_sequences = self.tokenizer.texts_to_sequences(self.train_df[self.feature_col])\n","        val_sequences = self.tokenizer.texts_to_sequences(self.val_df[self.feature_col])\n","\n","        # Padding sequences to a fixed length\n","        self.train_data = pad_sequences(train_sequences, maxlen=self.max_len, padding='post')\n","        self.val_data = pad_sequences(val_sequences, maxlen=self.max_len, padding='post')\n","\n","        # Extracting labels\n","        self.train_labels = self.train_df[self.label_col].values\n","        self.val_labels = self.val_df[self.label_col].values\n","\n","    def build_model(self):\n","        self.model = Sequential() # Initializing sequential model\n","        self.model.add(Embedding(len(self.tokenizer.word_index) + 1, 128, input_length=self.max_len)) # Adding Embedding layer\n","        self.model.add(LSTM(128)) # Adding LSTM layer\n","        self.model.add(Dense(64, activation='relu')) # Adding Dense layer\n","        self.model.add(Dense(1, activation='sigmoid')) # Adding Output layer\n","\n","        optimizer = Adam(learning_rate=self.learning_rate) # Initializing Adam optimizer\n","        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) # Compiling model\n","\n","    def train_model(self):\n","        self.history = self.model.fit(self.train_data, self.train_labels, epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.val_data, self.val_labels)) # Training the model\n","\n","    def get_training_loss(self):\n","        return self.history.history['loss']\n","\n","    def get_validation_loss(self):\n","        return self.history.history['val_loss']\n","\n","    def get_validation_accuracy(self):\n","        return self.history.history['val_accuracy']\n","\n","    def save_model(self, save_dir, model_name):\n","      os.makedirs(save_dir, exist_ok=True) # Creating directory if not exists\n","      self.model.save(os.path.join(save_dir, model_name + '.keras')) # Saving the model with .keras extension\n","\n","# Usage:\n","start_time = time.time()\n","model = 'lstm'\n","\n","## Hyperparameters\n","learning_rate = 2e-5\n","epochs = 3\n","batch_size = 6\n","max_len = 4096\n","optimizer = 'Adam' # TODO! Need to create the functionality to switch optimizers between Adam and AdamW\n","\n","## Paths and filenames\n","absolute_path = '/content/gdrive/My Drive/EmailSpam/'\n","train_file_path = 'Datasets/train_set.csv'\n","val_file_path = 'Datasets/validation_set.csv'\n","save_dir = 'TrainedModels/'\n","trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n","feature_col = 'Text'\n","label_col = 'Spam'\n","\n","trainer = LSTMTraining(learning_rate, epochs, batch_size, max_len, feature_col, label_col) # Creating instance of LSTMTraining class\n","trainer.load_data(absolute_path + train_file_path, absolute_path + val_file_path) # Loading data\n","trainer.preprocess_data() # Preprocessing data\n","trainer.build_model() # Building model\n","trainer.train_model() # Training model\n","trainer.save_model(absolute_path + save_dir, trained_model) # Saving trained model\n","\n","print(\"Training Loss:\", trainer.get_training_loss())\n","print(\"Validation Loss:\", trainer.get_validation_loss())\n","print(\"Validation Accuracy:\", trainer.get_validation_accuracy())\n","print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"]},{"cell_type":"markdown","source":["## Use the trained LSTM model to make predictions for a specific test set"],"metadata":{"id":"Vn5tJPMhstpd"}},{"cell_type":"code","source":["import pandas as pd\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from google.colab import drive\n","\n","class LSTMPredictions:\n","    def __init__(self, max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col):\n","        self.max_len = max_len\n","        self.test_file_path = test_file_path\n","        self.predictions_path = predictions_path\n","        self.absolute_path = absolute_path\n","        self.trained_model = trained_model\n","        self.feature_col = feature_col\n","        self.prediction_col = prediction_col\n","        drive.mount('/content/gdrive')  # Mount Google Drive to access files\n","\n","    def predict(self):\n","        # Load test dataset\n","        test_df = pd.read_csv(self.absolute_path + self.test_file_path)\n","        test_texts = test_df[self.feature_col].tolist()  # Extract the text data from the specified feature column\n","\n","        # Tokenize text data using the same tokenizer used during training\n","        tokenizer = Tokenizer()  # Initialize a Tokenizer object\n","        tokenizer.fit_on_texts(test_texts)  # Fit the tokenizer on the test text data\n","        test_sequences = tokenizer.texts_to_sequences(test_texts)  # Convert text data to sequences of integers\n","\n","        test_data = pad_sequences(test_sequences, maxlen=self.max_len, padding='post')  # Pad sequences to the maximum length specified during training\n","\n","        saved_model = load_model(self.absolute_path + self.trained_model)  # Load the trained LSTM model from the specified path\n","\n","        # Make predictions on test data using the loaded LSTM model\n","        predictions = saved_model.predict(test_data)  # Use the loaded LSTM model to make predictions on the test data\n","\n","        # Convert predictions to binary labels (0 or 1) based on a threshold (e.g., 0.5)\n","        binary_predictions = (predictions > 0.5).astype(int)\n","\n","        # Add the binary predictions as a new column to the test dataframe\n","        test_df[self.prediction_col] = binary_predictions\n","\n","        # Save predictions to CSV\n","        test_df.to_csv(self.absolute_path + self.predictions_path, index=False)\n","\n","        print(\"Predictions done\")\n","\n","# Usage:\n","max_len = 4096\n","str_params = 'lstm_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_4096'\n","\n","## Paths and filenames\n","absolute_path = '/content/gdrive/My Drive/EmailSpam/'\n","test_file_path = 'Datasets/test_set.csv'\n","predictions_path = 'Datasets/test_set_lstm.csv'\n","trained_model = 'TrainedModels/' + str_params + '.keras'\n","feature_col = 'Text'\n","prediction_col = str_params + '_prediction'\n","\n","# Instantiate the LSTMPredictions class\n","lstm_predictions = LSTMPredictions(max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col)\n","\n","# Perform predictions\n","lstm_predictions.predict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ApTFf0Ni3R6","executionInfo":{"status":"ok","timestamp":1714200869421,"user_tz":-180,"elapsed":35094,"user":{"displayName":"Roumeliotis Konstadinos","userId":"17264923090131634662"}},"outputId":"e96cfa89-5518-4b49-d13a-328b0dd06a9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","36/36 [==============================] - 3s 58ms/step\n","Predictions done\n"]}]}]}