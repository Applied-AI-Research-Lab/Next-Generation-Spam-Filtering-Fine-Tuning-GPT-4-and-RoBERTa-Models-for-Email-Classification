{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyO7dCQL4UDyyEPnf9KsKll/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Train a CNN to make predictions based on specific train and validation sets"],"metadata":{"id":"smPB1gwQtXCQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vjx60ntBha1W","executionInfo":{"status":"ok","timestamp":1714471651217,"user_tz":-180,"elapsed":71838,"user":{"displayName":"Roumeliotis Konstadinos","userId":"17264923090131634662"}},"outputId":"61dcae4a-d25f-45d8-c145-4b57924ceab7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 10627509576847154832\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 15510929408\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 14326477267733473123\n","physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n","xla_global_id: 416903419\n","]\n","Mounted at /content/gdrive\n","Epoch 1/3\n","595/595 [==============================] - 18s 23ms/step - loss: 0.5288 - accuracy: 0.8656 - val_loss: 0.4035 - val_accuracy: 0.8653\n","Epoch 2/3\n","595/595 [==============================] - 6s 10ms/step - loss: 0.3877 - accuracy: 0.8659 - val_loss: 0.3854 - val_accuracy: 0.8653\n","Epoch 3/3\n","595/595 [==============================] - 4s 7ms/step - loss: 0.3786 - accuracy: 0.8659 - val_loss: 0.3789 - val_accuracy: 0.8653\n","Training Loss: [0.5288362503051758, 0.38770896196365356, 0.3785526752471924]\n","Validation Loss: [0.4034520387649536, 0.3853952884674072, 0.37890899181365967]\n","Validation Accuracy: [0.8653198480606079, 0.8653198480606079, 0.8653198480606079]\n","Training time: 65.40 seconds\n"]}],"source":["import pandas as pd # For data manipulation\n","from tensorflow.keras.preprocessing.text import Tokenizer # For tokenizing text\n","from tensorflow.keras.optimizers import Adam # Adam optimizer for training\n","from tensorflow.keras.preprocessing.sequence import pad_sequences # For padding sequences\n","from tensorflow.keras.models import Sequential # Sequential model for building the neural network\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense # Layers for the neural network\n","from tensorflow.python.client import device_lib # A library for checking available devices in TensorFlow\n","import time\n","from google.colab import drive\n","import os\n","\n","# Check available devices\n","devices = device_lib.list_local_devices()\n","print(devices)\n","\n","class CNNTraining:\n","    def __init__(self, learning_rate, epochs, batch_size, max_len, feature_col, label_col):\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.max_len = max_len\n","        self.history = None\n","\n","    def load_data(self, train_file_path, val_file_path):\n","        drive.mount('/content/gdrive') # Mount Google Drive to access files\n","        train_df = pd.read_csv(train_file_path)\n","        val_df = pd.read_csv(val_file_path)\n","\n","        # Extracting text and labels from training data\n","        self.train_texts = train_df[feature_col].tolist()\n","        self.train_labels = train_df[label_col].values\n","\n","        # Extracting text and labels from validation data\n","        self.val_texts = val_df[feature_col].tolist()\n","        self.val_labels = val_df[label_col].values\n","\n","    def preprocess_data(self):\n","        self.tokenizer = Tokenizer() # Initializing Tokenizer\n","        self.tokenizer.fit_on_texts(self.train_texts) # Fitting tokenizer on training text data\n","\n","        # Converting text data to sequences\n","        train_sequences = self.tokenizer.texts_to_sequences(self.train_texts)\n","        val_sequences = self.tokenizer.texts_to_sequences(self.val_texts)\n","\n","        # Padding sequences to a fixed length\n","        self.train_data = pad_sequences(train_sequences, maxlen=self.max_len, padding='post')\n","        self.val_data = pad_sequences(val_sequences, maxlen=self.max_len, padding='post')\n","\n","    def build_model(self):\n","        self.model = Sequential() # Initializing sequential model\n","        self.model.add(Embedding(len(self.tokenizer.word_index) + 1, 128, input_length=self.max_len)) # Adding Embedding layer\n","        self.model.add(Conv1D(128, 5, activation='relu')) # Adding Convolutional layer\n","        self.model.add(GlobalMaxPooling1D()) # Adding GlobalMaxPooling layer\n","        self.model.add(Dense(64, activation='relu')) # Adding Dense layer\n","        self.model.add(Dense(1, activation='sigmoid')) # Adding Output layer\n","\n","        optimizer = Adam(learning_rate=self.learning_rate) # Initializing Adam optimizer\n","        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) # Compiling model\n","\n","    def train_model(self):\n","        self.history = self.model.fit(self.train_data, self.train_labels, epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.val_data, self.val_labels)) # Training the model\n","\n","    def get_training_loss(self):\n","        return self.history.history['loss']\n","\n","    def get_validation_loss(self):\n","        return self.history.history['val_loss']\n","\n","    def get_validation_accuracy(self):\n","        return self.history.history['val_accuracy']\n","\n","    def save_model(self, save_dir, model_name):\n","      os.makedirs(save_dir, exist_ok=True) # Creating directory if not exists\n","      self.model.save(os.path.join(save_dir, model_name + '.keras')) # Saving the model with .keras extension\n","\n","# Usage:\n","start_time = time.time()\n","model = 'cnn'\n","\n","## Hyperparameters\n","learning_rate = 2e-5\n","epochs = 3\n","batch_size = 6\n","max_len = 4096\n","optimizer = 'Adam' # TODO! Need to create the functionality to switch optimizers between Adam and AdamW\n","\n","## Paths and filenames\n","absolute_path = '/content/gdrive/My Drive/EmailSpam/'\n","train_file_path = 'Datasets/train_set.csv'\n","val_file_path = 'Datasets/validation_set.csv'\n","save_dir = 'TrainedModels/'\n","trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n","feature_col = 'Text'\n","label_col = 'Spam'\n","\n","trainer = CNNTraining(learning_rate, epochs, batch_size, max_len, feature_col, label_col) # Creating instance of CNNTraining class\n","trainer.load_data(absolute_path + train_file_path, absolute_path + val_file_path) # Loading data\n","trainer.preprocess_data() # Preprocessing data\n","trainer.build_model() # Building model\n","trainer.train_model() # Training model\n","trainer.save_model(absolute_path + save_dir, trained_model) # Saving trained model\n","\n","print(\"Training Loss:\", trainer.get_training_loss())\n","print(\"Validation Loss:\", trainer.get_validation_loss())\n","print(\"Validation Accuracy:\", trainer.get_validation_accuracy())\n","print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"]},{"cell_type":"markdown","source":["## Use the trained CNN model to make predictions for a specific test set"],"metadata":{"id":"Vn5tJPMhstpd"}},{"cell_type":"code","source":["import pandas as pd\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import os\n","from google.colab import drive\n","\n","class CNNPredictions:\n","    def __init__(self, max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col):\n","        self.max_len = max_len\n","        self.test_file_path = test_file_path\n","        self.predictions_path = predictions_path\n","        self.absolute_path = absolute_path\n","        self.trained_model = trained_model\n","        self.feature_col = feature_col\n","        self.prediction_col = prediction_col\n","        drive.mount('/content/gdrive') # Mount Google Drive to access files\n","\n","    def predict(self):\n","      # Load test dataset\n","      test_df = pd.read_csv(self.absolute_path + self.test_file_path)\n","      test_texts = test_df[self.feature_col].tolist()  # Extract the text data from the specified feature column\n","\n","      # Tokenize text data using the same tokenizer used during training\n","      tokenizer = Tokenizer()  # Initialize a Tokenizer object\n","      tokenizer.fit_on_texts(test_texts)  # Fit the tokenizer on the test text data\n","      test_sequences = tokenizer.texts_to_sequences(test_texts)  # Convert text data to sequences of integers\n","\n","      test_data = pad_sequences(test_sequences, maxlen=self.max_len, padding='post')  # Pad sequences to the maximum length specified during training\n","\n","      saved_model = load_model(self.absolute_path + self.trained_model)  # Load the trained model from the specified path\n","\n","      # Make predictions on test data using the loaded model\n","      predictions = saved_model.predict(test_data)  # Use the loaded model to make predictions on the test data\n","\n","      # Convert predictions to binary labels (0 or 1) based on a threshold (e.g., 0.5)\n","      binary_predictions = (predictions > 0.5).astype(int)\n","\n","      # Add the binary predictions as a new column to the test dataframe\n","      test_df[self.prediction_col] = binary_predictions\n","\n","      # Save predictions to CSV\n","      test_df.to_csv(self.absolute_path + self.predictions_path, index=False)\n","\n","      print(\"Predictions done\")\n","\n","# Usage:\n","max_len = 4096\n","str_params = 'cnn_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_4096'\n","\n","## Paths and filenames\n","absolute_path = '/content/gdrive/My Drive/EmailSpam/'\n","test_file_path = 'Datasets/test_set.csv'\n","predictions_path = 'Datasets/test_set.csv'\n","trained_model = 'TrainedModels/' + str_params + '.keras'\n","feature_col = 'Text'\n","prediction_col = str_params + '_prediction'\n","\n","# Instantiate the CNNPredictions class\n","cnn_predictions = CNNPredictions(max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col)\n","\n","# Perform predictions\n","cnn_predictions.predict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ApTFf0Ni3R6","executionInfo":{"status":"ok","timestamp":1714293410828,"user_tz":-180,"elapsed":5309,"user":{"displayName":"Roumeliotis Konstadinos","userId":"17264923090131634662"}},"outputId":"c6d6eb4a-af40-4f8f-8640-8d7fcfc7c54c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","35/35 [==============================] - 0s 6ms/step\n","Predictions done\n"]}]}]}